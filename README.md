Quantifying Rain Rates from geostationnary satellite Infrared radiometer using a Denoising Diffusion Probabilistic Model


This repository implements and evaluates diffusion models for precipitation prediction, with a focus on deep learning, visualization, and applied case studies.
The organization is designed to clearly separate the modelâ€™s core code, experiments, and exploratory analyses.

ğŸŒ Project Overview

This project aims to reconstruct precipitation fields from geostationary satellite infrared (IR) observations using deep generative models.
Unlike low-orbit satellites equipped with microwave sensors that can directly measure rainfall, geostationary satellites only provide IR measurements related to cloud-top temperatures. While this signal is indirect, it contains valuable information about cloud convection, vertical development, and organization, which are strongly correlated with precipitation events.

To capture these complex, nonlinear relationships, we explore the use of deep learning models, with a particular focus on diffusion models (DDPMs). These state-of-the-art generative architectures have shown impressive performance in image modeling and hold promising potential for precipitation estimation and nowcasting.


Code Structure:

Latmos_Precipitations_Diffusion
â”‚
â”œâ”€â”€ DL/                        
â”‚   â””â”€â”€ core/                  # Main scripts & core logic
â”‚       â”œâ”€â”€ eval.py
â”‚       â”œâ”€â”€ training.py
â”‚       â”œâ”€â”€ inference.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”‚
â”‚       â””â”€â”€ src/               # Submodules of the core model
â”‚           â”œâ”€â”€ Dataset/       # Data loading & transformations
â”‚           â”‚   â”œâ”€â”€ dataset.py
â”‚           â”‚   â”œâ”€â”€ transform.py
â”‚           â”‚   â””â”€â”€ csv/       # Normalization stats (mean/std)
â”‚           â”‚
â”‚           â”œâ”€â”€ Metrics/       # Evaluation & monitoring metrics
â”‚           â”‚   â”œâ”€â”€ eval_metrics.py
â”‚           â”‚   â”œâ”€â”€ metrics_function.py
â”‚           â”‚   â””â”€â”€ trainer_metrics.py
â”‚           â”‚
â”‚           â”œâ”€â”€ Sampling/      # Diffusion process & sampling
â”‚           â”‚   â”œâ”€â”€ diffusion_constants.py
â”‚           â”‚   â””â”€â”€ sample.py
â”‚           â”‚
â”‚           â”œâ”€â”€ Unet/          # U-Net architecture
â”‚           â”‚   â”œâ”€â”€ loss.py
â”‚           â”‚   â”œâ”€â”€ unet.py
â”‚           â”‚   â””â”€â”€ Unet_backbone/
â”‚           â”‚       â”œâ”€â”€ attention.py
â”‚           â”‚       â”œâ”€â”€ block.py
â”‚           â”‚       â”œâ”€â”€ sample.py
â”‚           â”‚       â””â”€â”€ time_embedding.py
â”‚           â”‚
â”‚           â””â”€â”€ Visualization/ # Visualization & plotting
â”‚               â”œâ”€â”€ eval_plot.py
â”‚               â”œâ”€â”€ plot.py
â”‚               â””â”€â”€ trainer_plot.py
â”‚
â”œâ”€â”€ build/                     
â”‚   â””â”€â”€ build.sh               # Build / run automation script
â”‚
â”œâ”€â”€ experiments/               # Saved experiments
â”‚   â”œâ”€â”€ best_experiment/       # Reference / best experiment
â”‚   â”‚   â”œâ”€â”€ checkpoint/
â”‚   â”‚   â”œâ”€â”€ configs/           # This is where the config files are stored, use them for Template <-------
â”‚   â”‚   â””â”€â”€ results/
â”‚   â”‚
â”‚   â””â”€â”€ experiment/            # General experiments
â”‚       â”œâ”€â”€ checkpoints/
â”‚       â”œâ”€â”€ configs/
â”‚       â”œâ”€â”€ results/
â”‚       â””â”€â”€ dump/
â”‚
â”œâ”€â”€ case_study/                # Specific case studies
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ core/              # core from DL
â”‚       â”œâ”€â”€ Matthieu/          # U-net by Matthieu Meignin
â”‚       â”‚   â””â”€â”€ unet_Matthieu.py
â”‚       â”œâ”€â”€ dataviz.py
â”‚       â”œâ”€â”€ generate_rain.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ tiles.py
â”‚
â”œâ”€â”€ EDA/                       # Exploratory Data Analysis
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ Matthieu/
â”‚       â”‚   â””â”€â”€ unet_Matthieu.py
â”‚       â”œâ”€â”€ data_analysis.py
â”‚       â”œâ”€â”€ data_treatment.py
â”‚       â”œâ”€â”€ fuse_dataset.py
â”‚       â””â”€â”€ nan_distribution.py
â”‚
â”œâ”€â”€ README.md                  # Project presentation
â”œâ”€â”€ Report.pdf                 # Project report
â”œâ”€â”€ requirements.txt  



ğŸ“¡ Data Source

The dataset is built from geostationary satellite (MSG metsat) IR radiometer measurements, which provide continuous, high-frequency coverage of large regions.

Input: Infrared brightness temperatures K (multiple IR channels) referenced as tb in the code.

Target: Rainfall estimates from meteofrance radar mosaic referenced as r in the code.

Challenge: The IRâ€“rainfall relationship is indirect and highly variable, requiring advanced inversion techniques to extract meaningful rainfall patterns.

This combination enables the training of data-driven models that can learn to infer precipitation from satellite imagery, bridging the gap between frequent IR observations and sparse direct rainfall measurements.

ğŸ§© Model Architecture
ğŸ”¹ General Structure

The model is based on a symmetric U-Net composed of three main parts:

ğŸ”½ Encoder (downsampling): progressively reduces spatial resolution while increasing feature depth using convolutions.

âš¡ Bottleneck: captures global patterns through ResNet blocks and attention modules.

ğŸ”¼ Decoder (upsampling): reconstructs the target image, reinjecting details via skip connections.

Each stage is enriched with:

â³ Temporal position embeddings (sinusoidal): inject noise-level information into the network.

ğŸ¯ Attention modules: capture long-range spatial dependencies, crucial for reconstructing large rainfall structures.

ğŸ”¹ Network Workflow

Encoder â†’ strided convolutions + ResNet blocks + attention â†’ compress input into abstract features.

Bottleneck â†’ 2 ResNet blocks + 1 attention block â†’ capture complex global structures.

Decoder â†’ upsampling + ResNet blocks + attention â†’ reconstruct output while leveraging skip connections for fine details.

Final layer â†’ predicts noise, which is subtracted at each diffusion step to generate the denoised image.

ğŸ”¹ Model Size

The U-Net totals â‰ˆ 9.9M parameters (~39.6 MB).

Component	Parameters
* Initial convolution	640
* Temporal MLP embeddings	82.4k
* Encoder blocks	1.6M
* Bottleneck â€“ ResNet (Ã—2)	1.3M each
* Bottleneck â€“ Attention	131k
* Decoder blocks	5.3M
* Final ResNet block	152k
* Output convolution	65

## ğŸ“– User Guide

### 1. Installation

* Create a virtual environment (recommended).
* Install all required Python packages:

pip install -r requirements.txt

### 2. Prepare the Dataset

* Dataset must contain `.npy` files:

  * **`rain_rate.npy`** â†’ target variable (precipitation rate).
  * **Condition file(s)** â†’ input data (can be named freely).
  * **`rain_quality.npy`** â†’ radar quality indicator (*optional*: not required for core functionality, but needed in some runs).

* Create CSV files for dataset splits (train/val/test).

  * Example scripts for generating splits are provided in the **`EDA/`** folder.

* Ensure the dataset contains **no NaN values**.

### 3. Configuration

* Define a configuration file in **YAML format** containing all training parameters.
* A **template config file** is available in the codebase for reference.

### 4. Build Script

* Create a build script (`.sh`) to launch training or evaluation.
* A **template build file** is included, adapted for the **hal.ipsl.fr** server.

### 5. Running the Code

The entry point is **`main.py`**, which supports three modes:

* ğŸ‹ï¸ **Training**:

  python DL/core/main.py --exp <exp_id> --mode train --config_dir <path_to_config>


* ğŸ“Š **Evaluation**:

  python DL/core/main.py --exp <exp_id> --mode eval --config_dir <path_to_config>


* ğŸ”® **Inference (on a single image)**:

  python DL/core/main.py --exp <exp_id> --mode inference --config_dir <path_to_config> --image_dir <path_to_image>



### ğŸ”‘ Arguments

* `--exp` â†’ experiment number (used to organize results in `experiments/`).
* `--mode` â†’ one of: `train`, `eval`, `inference`.
* `--config_dir` â†’ path to the YAML config file.
* `--image_dir` â†’ path to the input image (**only required for inference mode**).
* `--debug_run` â†’ add it for using the code on a small subset of the dataset

ğŸ‘¥ Contributors : Camille Francois Martin master student at Latmos, Meignin Matthieu Phd student at Latmos
